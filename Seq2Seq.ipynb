{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zthtOlIjcO1",
        "colab_type": "text"
      },
      "source": [
        "**Copyright(C). Cheonbok Park. All rights reserved.**\n",
        "\n",
        "Email : cb_park@korea.ac.kr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWPhKRrqw8Vl",
        "colab_type": "text"
      },
      "source": [
        "### 필요 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krquphUSw64W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 필요한 라이브러리를 설치합니다. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF6swkROGoRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch # torch library \n",
        "import torch.nn as nn # Nueral Network에 대한 package\n",
        "import numpy as np  # numpy \n",
        "import editdistance # 평가 지표로서 사용될 edit distance \n",
        "import matplotlib.pyplot as plt # plot 을 찍기 위한 라이브러리\n",
        "import tqdm\n",
        "import torch.nn.functional as F # pytorch function 들을 사용하기 위한 용도 \n",
        "from torch.utils import data # dataset 관련된 utility 를 사용하려는 용도\n",
        "from random import choice, randrange # random\n",
        "from itertools import zip_longest \n",
        "import librosa\n",
        "import os   # directory 생성 및 디렉토리 생성과 관련된 package \n",
        "import json \n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvgO0Z-3xKpr",
        "colab_type": "text"
      },
      "source": [
        "### Data Loader "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5VY4V8ifgNm",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://pytorch.org/tutorials/_images/word-encoding.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq545uVj3nUv",
        "colab_type": "text"
      },
      "source": [
        "i go to the school\n",
        "\n",
        "1 7 21 3 50 +0 (길이 맞춰주려고 0 추가)\n",
        "\n",
        "i have to read book\n",
        "\n",
        "1 11 8 27 16 31"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPt35iPSs0dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch(iterable, n=1):\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(*args)\n",
        "\n",
        "\n",
        "def pad_tensor(vec, pad, value=0, dim=0):\n",
        "    \"\"\"\n",
        "    pad token으로 채우는 용도 \n",
        "    args:\n",
        "        vec - tensor to pad\n",
        "        pad - the size to pad to\n",
        "        dim - dimension to pad\n",
        "    return:\n",
        "        a new tensor padded to 'pad' in dimension 'dim'\n",
        "    \"\"\"\n",
        "    pad_size = pad - vec.shape[0]\n",
        "\n",
        "    if len(vec.shape) == 2:\n",
        "        zeros = torch.ones((pad_size, vec.shape[-1])) * value\n",
        "    elif len(vec.shape) == 1:\n",
        "        zeros = torch.ones((pad_size,)) * value\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return torch.cat([torch.Tensor(vec), zeros], dim=dim)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t-W0gQps2rg",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.stack.imgur.com/Kuhh0.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAaKIfA7BdLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_collate(batch, values=(0, 0), dim=0):\n",
        "    \"\"\"\n",
        "    데이터 로더에 들어가기전에 batch화 할 때 거치는 함수 \n",
        "    args:\n",
        "        batch - list of (tensor, label)\n",
        "    reutrn:\n",
        "        xs - a tensor of all examples in 'batch' after padding\n",
        "        ys - a LongTensor of all labels in batch\n",
        "        ws - a tensor of sequence lengths\n",
        "    \"\"\"\n",
        "\n",
        "    sequence_lengths = torch.Tensor([int(x[0].shape[dim]) for x in batch]) # 각 batch 마다 길이를 얻어내고 \n",
        "    sequence_lengths, xids = sequence_lengths.sort(descending=True) # 감소하는 순서로 정렬\n",
        "    target_lengths = torch.Tensor([int(x[1].shape[dim]) for x in batch])\n",
        "    # find longest sequence (가장 긴 sequence의 길이를 구함 )\n",
        "    src_max_len = max(map(lambda x: x[0].shape[dim], batch))\n",
        "    tgt_max_len = max(map(lambda x: x[1].shape[dim], batch))\n",
        "    # pad according to max_len (max length 만큼 padd를 추가 )\n",
        "    batch = [(pad_tensor(x, pad=src_max_len, dim=dim), pad_tensor(y, pad=tgt_max_len, dim=dim)) for (x, y) in batch]\n",
        "\n",
        "    # stack all #length same\n",
        "    xs = torch.stack([x[0] for x in batch], dim=0)\n",
        "    ys = torch.stack([x[1] for x in batch], dim=0) #descendig order 아직 아님\n",
        "    xs = xs[xids].contiguous() # decreasing order로 다시 나열 \n",
        "    ys = ys[xids].contiguous() # xids 와 같은 순서로 \n",
        "    target_lengths = target_lengths[xids] #순서 정렬\n",
        "    return xs.long(), ys.long(), sequence_lengths.int(), target_lengths.int()\n",
        "\n",
        "\n",
        "class ToyDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    https://talbaumel.github.io/blog/attention/\n",
        "    \"\"\"\n",
        "    def __init__(self, min_length=5, max_length=20, type='train'):\n",
        "        self.SOS = \"<s>\"  # all strings will end with the End Of String token )\n",
        "        self.EOS = \"</s>\"  # all strings will end with the End Of String token\n",
        "        self.characters = list(\"abcde\")  #abcdefg\n",
        "        self.int2char = list(self.characters)\n",
        "        self.char2int = {c: i+3 for i, c in enumerate(self.characters)} # +3 을 왜하는 가?\n",
        "        print(self.char2int)\n",
        "        self.VOCAB_SIZE = len(self.characters)\n",
        "        self.min_length = min_length\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # train set or test set 을 생성 \n",
        "        if type == 'train':\n",
        "            self.set = [self._sample() for _ in range(4000)]\n",
        "        else:\n",
        "            self.set = [self._sample() for _ in range(300)]\n",
        "\n",
        "    def __len__(self):\n",
        "        # 필수 ! \n",
        "        return len(self.set)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        # 필수 !\n",
        "        return self.set[item]\n",
        "\n",
        "    def _sample(self):\n",
        "        random_length = randrange(self.min_length, self.max_length)  # Pick a random length\n",
        "        random_char_list = [choice(self.characters[:-1]) for _ in range(random_length)]  # Pick random chars\n",
        "        random_string = ''.join(random_char_list)\n",
        "        a = np.array([self.char2int.get(x) for x in random_string]+[2])\n",
        "        b = np.array([self.char2int.get(x) for x in random_string[::-1]] + [2]) # Return the random string and its reverse + EOS \n",
        "        \n",
        "        return a, b\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QcWCIsJzltc",
        "colab_type": "text"
      },
      "source": [
        "### utils misc.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SciyvlCzBe-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EOS_TOKEN = '</s>'\n",
        "\n",
        "\n",
        "def check_size(tensor, *args):\n",
        "    size = [a for a in args]\n",
        "    assert tensor.size() == torch.Size(size), tensor.size()\n",
        "\n",
        "def to_mono(y):\n",
        "    assert y.ndim == 2\n",
        "    return np.mean(y, axis=1)\n",
        "\n",
        "\n",
        "def edit_distance(guess, truth):\n",
        "    guess = guess.split(EOS_TOKEN)[0]\n",
        "    truth = truth[3:].split(EOS_TOKEN)[0]\n",
        "    return editdistance.eval(guess, truth) / len(truth)\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "  __getattr__ = dict.__getitem__\n",
        "  __setattr__ = dict.__setitem__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Hv4JFBzM-4",
        "colab_type": "text"
      },
      "source": [
        "### Edit distance (편집거리 알고리즘) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4ct9e_OzuTT",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://raw.githubusercontent.com/sumitc91/data/master/askgif-blog/9e07d056-ccf7-4fc8-b6ee-000c8032b9ec_editDistance.gif)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwbk7k7h4PKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b937bf0-21ed-4bed-e9e0-f401328b6b75"
      },
      "source": [
        "# edit distance 란 편집 거리 \n",
        "\n",
        "\n",
        "ref = [1, 2, 3, 4]\n",
        "hyp = [1, 2, 4, 5, 6]\n",
        "editdistance.eval(ref,hyp)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1VtDggGzzs_",
        "colab_type": "text"
      },
      "source": [
        "### Attention Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOMNQN6Gj_-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 추후에 설명 Decoder section\n",
        "def mask_3d(inputs, seq_len, mask_value=0.):\n",
        "    batches = inputs.size()[0]\n",
        "    assert batches == len(seq_len) # length 체크 \n",
        "    max_idx = max(seq_len) # max length 체크 \n",
        "    for n, idx in enumerate(seq_len): # length 에서 의미없는 hidden state attention 값은 0으로 두기 위한 mask값 설정 \n",
        "        if idx < max_idx.item():\n",
        "            if len(inputs.size()) == 3:\n",
        "                inputs[n, idx.int():, :] = mask_value\n",
        "            else:\n",
        "                assert len(inputs.size()) == 2, \"The size of inputs must be 2 or 3, received {}\".format(inputs.size())\n",
        "                inputs[n, idx.int():] = mask_value\n",
        "    return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXtIVGUgHmws",
        "colab_type": "text"
      },
      "source": [
        "## Encoder RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFDvktffm9k",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf62OoGcwJP5",
        "colab_type": "text"
      },
      "source": [
        "#### Embedding Module "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8mNg3ve8LmT",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.ibb.co/S0NJzq7/embedding.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH2OnZrXwOqk",
        "colab_type": "text"
      },
      "source": [
        "#### GRU Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goSiSeiq8bn-",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.ibb.co/881BygH/GRU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_TIF6h58dZf",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.ibb.co/NsMqvcH/GRU-param.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Z_vgd2wSAd",
        "colab_type": "text"
      },
      "source": [
        "#### ENCODER RNN Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d__61m2HfFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.input_size = config[\"n_channels\"]\n",
        "        self.hidden_size = config[\"encoder_hidden\"]\n",
        "        self.layers = config.get(\"encoder_layers\", 1)\n",
        "        \n",
        "        self.dropout = config.get(\"encoder_dropout\", 0.) \n",
        "        self.bi = config.get(\"bidirectional_encoder\", False)\n",
        "        embedding_dim = config.get(\"embedding_dim\", None)\n",
        "        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n",
        "        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0) #임베딩 할 때 패드 토큰으로 0 투입\n",
        "        gru_input_dim = self.embedding_dim\n",
        "        self.rnn = nn.GRU(\n",
        "            gru_input_dim,\n",
        "            self.hidden_size,\n",
        "            self.layers,\n",
        "            dropout=self.dropout,\n",
        "            bidirectional=self.bi,\n",
        "            batch_first=True)# model 선언 \n",
        "        self.gpu = config.get(\"gpu\", False) \n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs, hidden, input_lengths):\n",
        "        \n",
        "        # pack padded 를 통하여 input을 감싸기 \n",
        "        inputs = self.embedding(inputs)\n",
        "        \n",
        "        x = pack_padded_sequence(inputs, input_lengths, batch_first=True) \n",
        "        output, state = self.rnn(x, hidden)\n",
        "        output, _ = pad_packed_sequence(output, batch_first=True, padding_value=0.) # sequence 를 위의 그림과 같이 pack함 # matrix화\n",
        "        \n",
        "        if self.bi: # bidirectional 의 경우 forward와 backward를 sum하여 사용한다. or concat \n",
        "            output = output[:, :, :self.hidden_size] + output[:, :, self.hiddensize:]\n",
        "            state = state[:1] +state[1:]\n",
        "        return output, state\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # hidden state가 없는 초기 상태일때 \n",
        "        h0 = torch.zeros(2 if self.bi else 1, batch_size, self.hidden_size)\n",
        "        if self.gpu:\n",
        "            h0 = h0.cuda()\n",
        "        return h0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZSYZqWVwM_z",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://pytorch.org/tutorials/_images/attention-decoder-network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8yDzSXKHnyB",
        "colab_type": "text"
      },
      "source": [
        "### Decoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X83Lj_GBHojW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#decoder쪽은 bidation 하면 안됨(옆에를 copy 해버림) (무조건 redirection)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = config[\"batch_size\"]\n",
        "        self.hidden_size = config[\"decoder_hidden\"]\n",
        "        \n",
        "        embedding_dim = config.get(\"embedding_dim\", None)\n",
        "        self.embedding_dim = embedding_dim if embedding_dim is not None else self.hidden_size\n",
        "        self.embedding = nn.Embedding(config.get(\"n_classes\", 32), self.embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=self.embedding_dim+self.hidden_size if config['decoder'].lower() == 'bahdanau' else self.embedding_dim,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=config.get(\"decoder_layers\", 1),\n",
        "            dropout=config.get(\"decoder_dropout\", 0),\n",
        "            bidirectional=False, #decoder 안에서는 어떠한 경우라도 False\n",
        "            batch_first=True)\n",
        "        if config['decoder'] != \"RNN\":\n",
        "            self.attention = Attention(\n",
        "                self.batch_size,\n",
        "                self.hidden_size,\n",
        "                method=config.get(\"attention_score\", \"dot\"))\n",
        "\n",
        "        self.gpu = config.get(\"gpu\", False)\n",
        "        self.decoder_output_fn = F.log_softmax if config.get('loss', 'NLL') == 'NLL' else None\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        \"\"\" Must be overrided \"\"\"\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcUzMxqV9Lr2",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.ibb.co/gvpn1RT/bmm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBkw-82hE3MU",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](http://cnyah.com/2017/08/01/attention-variants/attention-mechanisms.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYbzCRP6HuLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauDecoder(Decoder):\n",
        "    \"\"\"\n",
        "        Corresponds to BahdanauAttnDecoderRNN in Pytorch tutorial\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BahdanauDecoder, self).__init__(config)\n",
        "        self.output_size = config.get(\"n_classes\", 32)\n",
        "        self.character_distribution = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        \"\"\"\n",
        "        :param input: [B]\n",
        "        :param prev_context: [B, H]\n",
        "        :param prev_hidden: [B, H]\n",
        "        :param encoder_outputs: [B, T, H]\n",
        "        :return: output (B), context (B, H), prev_hidden (B, H), weights (B, T)\n",
        "        \"\"\"\n",
        " \n",
        "        input = kwargs[\"input\"] # decoder input \n",
        "        prev_hidden = kwargs[\"prev_hidden\"] # decoder rnn 에서 들어갈 previous hidden state \n",
        "        encoder_outputs = kwargs[\"encoder_outputs\"] # encoder RNN에서 Encoding이 끝난 (B,L,hidden_size)  \n",
        "        seq_len = kwargs.get(\"seq_len\", None) # sequence length \n",
        "\n",
        "        # check inputs\n",
        "        \n",
        "       \n",
        "\n",
        "        # Attention weights\n",
        "        weights = self.attention.forward(prev_hidden, encoder_outputs, seq_len)  # B x T\n",
        "        context = weights.unsqueeze(1).bmm(encoder_outputs).squeeze(1)  #[B,1,T] [B,T,H] -> [B x H]\n",
        "\n",
        "        # embed characters\n",
        "        embedded = self.embedding(input).unsqueeze(0) #(B,EMBDDing) -> (1,B,EMBEdding)\n",
        "        \n",
        "        #attention 을 통해 얻어낸 context를 추가하여 모델에 input으로 제공\n",
        "        rnn_input = torch.cat( (embedded, context.unsqueeze(0) ) , 2) #(1,B,embedding (1,B,B)) -> (1,B,embedding+B)\n",
        "        #(1,b,emb+h) -> .transpose(1,0) -> (b,1,emb+h)\n",
        "        outputs, hidden = self.rnn(rnn_input.transpose(1, 0), prev_hidden.unsqueeze(0)) # 1 x B x N, B x N\n",
        "\n",
        "        #(b,1,hiddn_size) -> (b,logits)\n",
        "        output = self.character_distribution(outputs.squeeze(0)) # logit 값 각 chracter 별로\n",
        "\n",
        "        if self.decoder_output_fn:\n",
        "            # NLL loss 인 경우 \n",
        "            output = self.decoder_output_fn(output, -1)\n",
        "\n",
        "        if len(output.size()) == 3:\n",
        "            output = output.squeeze(1)\n",
        "\n",
        "        return output, hidden.squeeze(0), weights\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhtZXhUEQ64",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.stack.imgur.com/tiQkz.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYfGqtu0eqZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        last_hidden: (batch_size, hidden_size)\n",
        "        encoder_outputs: (batch_size, max_time, hidden_size)\n",
        "    Returns:\n",
        "        attention_weights: (batch_size, max_time)\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, hidden_size, method=\"dot\"):\n",
        "        super(Attention, self).__init__()\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "        if method == 'dot':\n",
        "            pass\n",
        "        elif method == 'general':\n",
        "            # Wa (hidden,hidden)\n",
        "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        elif method == \"concat\":\n",
        "            # Wa : (2*hidden,hidden)\n",
        "            # Va : (hidden,1)\n",
        "            self.Wa = nn.Linear(2*hidden_size, hidden_size, bias=False)\n",
        "            self.va = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n",
        "        elif method == 'bahdanau':\n",
        "            # Wa : (hidden_size,hidden_size) \n",
        "            # Ua : (hidden_size,hidden_size)\n",
        "            # Va : (hidden_size,1)\n",
        "            self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "            self.va = nn.Parameter(torch.FloatTensor(hidden_size, 1))\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        \n",
        "    def forward(self, last_hidden, encoder_outputs, seq_len=None):\n",
        "        \"\"\"\n",
        "        Inputs :\n",
        "          last_hidden : (B,T,hidden_size)\n",
        "          encoder_outputs : \n",
        "          seq_len:  \n",
        "        Returns:\n",
        "          attention matrix : \n",
        "        \"\"\"\n",
        "        batch_size, seq_lens, _ = encoder_outputs.size()\n",
        "        # attention energies 를 구하기 \n",
        "        attention_energies = self._score(last_hidden, encoder_outputs, self.method)\n",
        "        \n",
        "        if seq_len is not None:\n",
        "            attention_energies = mask_3d(attention_energies, seq_len, -float('inf'))\n",
        "\n",
        "        return F.softmax(attention_energies, -1)\n",
        "\n",
        "    def _score(self, last_hidden, encoder_outputs, method):\n",
        "        \"\"\"\n",
        "        Computes an attention score\n",
        "        :param last_hidden: (batch_size, hidden_dim)\n",
        "        :param encoder_outputs: (batch_size, max_time, hidden_dim)\n",
        "        :param method: str (`dot`, `general`, `concat`)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        # assert last_hidden.size() == torch.Size([batch_size, self.hidden_size]), last_hidden.size()\n",
        "        \n",
        "        if method == 'dot':\n",
        "            last_hidden = last_hidden.unsqueeze(-1) # (batch_size, hidden_dim,1)\n",
        "            \n",
        "            # attention : (batch_size,max_time, hidden_dim) , (batch_size,hidden_dim,1) - > (batch_size,max_time ,1)\n",
        "            \n",
        "            return encoder_outputs.bmm(last_hidden).squeeze(-1)  \n",
        "\n",
        "        elif method == 'general':\n",
        "            # dot 이랑 비슷 다만 last hidden을 한번 projection\n",
        "            x = self.Wa(last_hidden) # (batch_size, hidden_dim) ->  (batch_size, hidden_dim)\n",
        "            x = x.unsqueeze(-1) # (batch_size, hidden_dim) ->  (batch_size, hidden_dim,1)\n",
        "            # encoded 된 hidden states 와 dot proudct를 수행하기 \n",
        "            # attention: (batch_size,max_time, hidden_dim) , (batch_size,hidden_dim,1) - > (batch_size,max_time ,1)\n",
        "            return encoder_outputs.bmm(x).squeeze(-1)\n",
        "\n",
        "        elif method == \"concat\":\n",
        "            x = last_hidden.unsqueeze(1).expand_as(encoder_outputs) # (batch_size, hidden_dim) ->  unsqueeze -> (batch_size,1, hidden_dim) -> b, max_time, hidden_state\n",
        "            # concat 후 -> linear 거치기 -> 후 tanh\n",
        "            x = F.tanh(self.Wa(torch.cat((x, encoder_outputs), -1))) # (batch_size, max_timestep, hidden_dim) ->  (batch_size,  max_timestep, hidden_dim*2)\n",
        "            # (batch_size, max_timestep, hidden_dim*2) ->  (batch_size,  max_timestep, 1) -> squeeze[-1] -> [b,max....]\n",
        "            return x.matmul(self.va).squeeze(-1)\n",
        "\n",
        "        elif method == \"bahdanau\":\n",
        "            # mlp 기반의 attention model\n",
        "            \n",
        "            x = last_hidden.unsqueeze(1) # (batch_size, hidden_dim) ->  (batch_size,1, hidden_dim)\n",
        "            # 각각을 projection 후 더하기 -> tanh \n",
        "            out = F.tanh(self.Wa(x) + self.Ua(encoder_outputs)) # \n",
        "            return out.matmul(self.va).squeeze(-1)# (batch_size,max_timestep,hidden_dim) ->  (batch_size, max_timestep)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqeFcsDJ7Dz2",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seq Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS2cmY548vtB",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.ibb.co/CK5wTz5/crossentropy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_rIN2DY80pD",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://i.ibb.co/ssXZ28q/crossentropy-2.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-axgHRo7C5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "        Sequence to sequence module\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.SOS = config.get(\"start_index\", 1) # Start index를 가져옵니다. \n",
        "        self.vocab_size = config.get(\"n_classes\", 32) # embedding 에 필요한 vocabulary size \n",
        "        self.batch_size = config.get(\"batch_size\", 1) # batch_size 정보를 가져옵니다.\n",
        "        self.gpu = config.get(\"gpu\", False) # cuda 로 돌아가는지 아닌지에 대한 정보 \n",
        "\n",
        "        # Encoder 선언\n",
        "        \n",
        "        self.encoder = EncoderRNN(config)\n",
        "\n",
        "        # Decoder 선언 \n",
        "        \n",
        "        self.decoder = BahdanauDecoder(config)\n",
        "        \n",
        "        # loss fucntion \n",
        "        # ignore_index =0 왜???\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "        \n",
        "        \n",
        "\n",
        "    def encode(self, x, x_len):\n",
        "        # encoder를 통해 주어진 source 정보를 Encodeing 하는 용도 \n",
        "        \n",
        "        batch_size = x.size()[0]\n",
        "        # 초기 inital hidden state 만들기\n",
        "        init_state = self.encoder.init_hidden(batch_size)\n",
        "        # encoder Forward 수행 \n",
        "        encoder_outputs, encoder_state = self.encoder.forward(x, init_state, x_len)\n",
        "        \n",
        "        \n",
        "       \n",
        "        return encoder_outputs, encoder_state.squeeze(0)\n",
        "\n",
        "    def decode(self, encoder_outputs, encoder_hidden, targets, targets_lengths, input_lengths):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            encoder_outputs: (B, T, H)\n",
        "            encoder_hidden: (B, H)\n",
        "            targets: (B, L)\n",
        "            targets_lengths: (B)\n",
        "            input_lengths: (B)\n",
        "        Vars:\n",
        "            decoder_input: (B)\n",
        "            decoder_context: (B, H)\n",
        "            hidden_state: (B, H)\n",
        "            attention_weights: (B, T)\n",
        "        Outputs:\n",
        "            alignments: (L, T, B)\n",
        "            logits: (B*L, V)\n",
        "            labels: (B*L)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_outputs.size()[0]\n",
        "        max_length = targets.size()[1]\n",
        "        # decoder의 처음 y0 는 무엇이 되어야 할까? *주의해야할 포인트 \n",
        "        if batch_size ==1:\n",
        "          decoder_input = torch.LongTensor([self.SOS] * batch_size)\n",
        "        else:\n",
        "          decoder_input = torch.LongTensor([self.SOS] * batch_size).squeeze(-1)\n",
        "        decoder_context = encoder_outputs.transpose(1, 0)[-1] #(Batch,1)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        \n",
        "        #alignments :  attention align을 저장하기 위한 용도  \n",
        "        alignments = torch.zeros(max_length, encoder_outputs.size(1), batch_size) # attention align을 저장하기 위한 용도 \n",
        "        logits = torch.zeros(max_length, batch_size, self.decoder.output_size) # logits 값을 저장하기 위한 용도의 tensor \n",
        "\n",
        "        if self.gpu:\n",
        "            decoder_input = decoder_input.cuda()\n",
        "            decoder_context = decoder_context.cuda()\n",
        "            logits = logits.cuda()\n",
        "        inference = []\n",
        "        for t in range(max_length):\n",
        "\n",
        "            # The decoder accepts, at each time step t :\n",
        "            # - an input, [B]\n",
        "            # - a context, [B, H]\n",
        "            # - an hidden state, [B, H]\n",
        "            # - encoder outputs, [B, T, H]\n",
        "            \n",
        "            # The decoder outputs, at each time step t :\n",
        "            # - an output, [B]\n",
        "            # - a context, [B, H]\n",
        "            # - an hidden state, [B, H]\n",
        "            # - weights, [B, T]\n",
        "\n",
        "            outputs, decoder_hidden, attention_weights = self.decoder.forward(\n",
        "                    input=decoder_input.long(),\n",
        "                    prev_hidden=decoder_hidden,\n",
        "                    encoder_outputs=encoder_outputs,\n",
        "                    seq_len=input_lengths)\n",
        "            \n",
        "            alignments[t] = attention_weights.transpose(1, 0)\n",
        "            \n",
        "            \n",
        "            logits[t] = outputs\n",
        "\n",
        "            \n",
        "\n",
        "            if  self.training:\n",
        "                decoder_input = targets[:, t]\n",
        "            else:\n",
        "                topv, topi = outputs.data.topk(1) # 가장 높은 예측만 사용.\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "                inference.append(decoder_input.cpu())\n",
        "\n",
        "        \n",
        "        labels = targets.contiguous().view(-1) \n",
        "\n",
        "        \n",
        "        mask_value = 0\n",
        "        #what is this mask_3d? # (warning check)\n",
        "        logits = mask_3d(logits.transpose(1, 0), targets_lengths, mask_value)\n",
        "        logits = logits.contiguous().view(-1, self.vocab_size) # loss를 구하기 위해 쫙 펴주기 \n",
        "\n",
        "        return logits, labels.long(), alignments,inference\n",
        "\n",
        "    \n",
        "    def step(self, batch):\n",
        "        x, y, x_len, y_len = batch\n",
        "        if self.gpu:\n",
        "            x = x.cuda()\n",
        "            y = y.cuda()\n",
        "            x_len = x_len.cuda()\n",
        "            y_len = y_len.cuda()\n",
        "\n",
        "        encoder_out, encoder_state = self.encode(x, x_len) # encoder \n",
        "        logits, labels, alignments,inference = self.decode(encoder_out, encoder_state, y, y_len, x_len) # decoder 를 통해 alignment와 logit 값 얻기 \n",
        "        return logits, labels, alignments,inference\n",
        "\n",
        "    def loss(self, batch):\n",
        "        logits, labels, alignments,inference = self.step(batch)\n",
        "        loss = self.loss_fn(logits, labels) # loss 구하기 우리는 cross entropy 사용 \n",
        "        return loss, logits, labels, alignments,inference"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbOfjOmU6262",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weB4hLTwfV3S",
        "colab_type": "text"
      },
      "source": [
        "![대체 텍스트](https://pytorch.org/tutorials/_images/seq2seq.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKYVKohKBmpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, epoch,n_epochs):\n",
        "    \n",
        "\n",
        "    losses = []\n",
        "    cers = []\n",
        "\n",
        "    \n",
        "    model.train() # train mode \n",
        "    count = 0\n",
        "    for batch in train_loader:\n",
        "        loss, _, _, _,_ = model.loss(batch)\n",
        "        losses.append(loss.item())\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2)\n",
        "        optimizer.step()\n",
        "  \n",
        "    print ('\\n [{}/{}] avg_loss= {:05.3f}'.format(epoch,n_epochs,np.mean(losses)))\n",
        "    \n",
        "    return model, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0q5dl-qGtXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, eval_loader):\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "    edits = []\n",
        "    \n",
        "    model.eval() # why?? \n",
        "    #with 부분을 이용해서 메모리 저장 안하게 함\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_loader:\n",
        "            #t.set_description(\" Evaluating... (train={})\".format(model.training))\n",
        "            loss, logits, labels, alignments,_ = model.loss(batch)\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            \n",
        "            acc = 100 *np.sum(np.argmax(preds, -1) == labels.detach().cpu().numpy()) / len(preds)\n",
        "            edit = editdistance.eval(np.argmax(preds, -1), labels.detach().cpu().numpy()) / len(preds)\n",
        "            \n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            accs.append(acc)\n",
        "            edits.append(edit)\n",
        "        \n",
        "        \n",
        "\n",
        "   \n",
        "    print(\"  End of evaluation : loss {:05.3f} , acc {:03.1f} , edits {:03.3f}\".format(np.mean(losses), np.mean(accs), np.mean(edits)))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWWvvbf68__6",
        "colab_type": "text"
      },
      "source": [
        "## 학습을 진행해보도록 하겠습니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6zoFKQj8_MU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e83a52b8-fd81-41f3-af38-a94b7da25d5f"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "batch_size = 32\n",
        "epochs = 6\n",
        "dataset = ToyDataset(5, 15)\n",
        "eval_dataset = ToyDataset(5, 15, type='eval')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7}\n",
            "{'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKechkHv9EXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate, drop_last=True)\n",
        "eval_loader = data.DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate,drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JapBBgq6KNp2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2HQkg59sND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "  \"decoder\": \"Bahdanau\",\n",
        "  \"encoder\": \"RNN\",\n",
        "  \"n_channels\": 5,\n",
        "  \"encoder_hidden\": 64,\n",
        "  \"encoder_layers\": 1,\n",
        "  \"encoder_dropout\": 0.2,\n",
        "  \"bidirectional_encoder\": False,\n",
        "  \"decoder_hidden\": 64,\n",
        "  \"decoder_layers\": 1,\n",
        "  \"decoder_dropout\": 0.2,\n",
        "  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n",
        "  \"batch_size\": 32,\n",
        "  \"embedding_dim\": 64,\n",
        "  \"attention_score\": \"concat\",\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"gpu\": True,\n",
        "  \"loss\": \"cross_entropy\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_h_LOmU-AEz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "900aae52-13ef-48cf-95c4-5c3009a7b1dc"
      },
      "source": [
        "model = Seq2Seq(config)\n",
        "model = model.cuda()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jXTGn9V-CuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ktMBioF-EAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "650da56b-c416-4566-e285-7c70db5eff5c"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n",
        "  evaluate(model,eval_loader)\n",
        " "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " [0/6] avg_loss= 1.273\n",
            "  End of evaluation : loss 1.208 , acc 62.2 , edits 0.298\n",
            "\n",
            " [1/6] avg_loss= 0.661\n",
            "  End of evaluation : loss 0.861 , acc 79.3 , edits 0.166\n",
            "\n",
            " [2/6] avg_loss= 0.427\n",
            "  End of evaluation : loss 0.657 , acc 86.7 , edits 0.109\n",
            "\n",
            " [3/6] avg_loss= 0.267\n",
            "  End of evaluation : loss 0.618 , acc 88.1 , edits 0.093\n",
            "\n",
            " [4/6] avg_loss= 0.165\n",
            "  End of evaluation : loss 0.397 , acc 94.0 , edits 0.041\n",
            "\n",
            " [5/6] avg_loss= 0.152\n",
            "  End of evaluation : loss 0.191 , acc 97.2 , edits 0.022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RNRT6acajT5",
        "colab_type": "text"
      },
      "source": [
        "### 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ77ZlPKal2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn\n",
        "\n",
        "def draw(data, x, y):\n",
        "    seaborn.heatmap(data, \n",
        "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
        "                    cbar=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3fVzxjganHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualize_plot(model,custom_input= 'cgdafa'):\n",
        "    c_xs = np.array([dataset.char2int.get(x) for x in custom_input]+[2])\n",
        "    c_xs = torch.from_numpy(c_xs).unsqueeze(0).long()\n",
        "\n",
        "    c_xl = torch.tensor(c_xs[0].size()[-1]).unsqueeze(0)\n",
        "\n",
        "    c_ys = np.array([dataset.char2int.get(x) for x in custom_input[::-1]] + [2]) # Return the random string and its reverse + EOS \n",
        "    c_ys = torch.from_numpy(c_ys).unsqueeze(0).long()\n",
        "\n",
        "    c_yl = torch.tensor(c_ys[0].size()[-1]).unsqueeze(0)\n",
        "    c_data = (c_xs,c_ys,c_xl,c_yl)\n",
        "    loss, logits, labels, alignments,predict=model.loss(c_data)\n",
        "    heat_map_value = alignments.detach().cpu().numpy()[:, :, 0]\n",
        "    preds = logits.detach().cpu().numpy()\n",
        "    preds = np.argmax(preds, -1)\n",
        "    source_tokens = [ dataset.int2char[item-3] for item in c_xs[0] if item!=0 if item !=2 ] +['</s>']\n",
        "    target_tokens = [ dataset.int2char[item-3] if item !=2 else '</s>' for item in preds.tolist() if item!=0 ]\n",
        "    draw(heat_map_value,source_tokens,target_tokens)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IkbDmwaY4BG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "c642f721-455c-4ce7-a3a2-0bf1f0695416"
      },
      "source": [
        "visualize_plot(model,'cbada') #왼쪽에 있는게 생성, 밑에 있는게 참조한 것"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD8CAYAAABTq8lnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlxoxeKmR/wUPb7x43eTVcAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZdAsEpjcC0V",
        "colab_type": "text"
      },
      "source": [
        "### Dot Mode "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8X3wWrrewkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "  \"decoder\": \"Bahdanau\",\n",
        "  \"encoder\": \"RNN\",\n",
        "  \"n_channels\": 4,\n",
        "  \"encoder_hidden\": 64,\n",
        "  \"encoder_layers\": 1,\n",
        "  \"encoder_dropout\": 0.2,\n",
        "  \"bidirectional_encoder\": False,\n",
        "  \"decoder_hidden\": 64,\n",
        "  \"decoder_layers\": 1,\n",
        "  \"decoder_dropout\": 0.2,\n",
        "  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n",
        "  \"batch_size\": 32,\n",
        "  \"embedding_dim\": 64,\n",
        "  \"attention_score\": \"concat\",\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"gpu\": True,\n",
        "  \"loss\": \"cross_entropy\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xHTy6m9k7XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq(config)\n",
        "model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuAGn6wOcMAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NNv1KexcOQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n",
        "  evaluate(model,eval_loader)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48ACZ__zcgz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_plot(model,'cbada')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZio7ZpScXWf",
        "colab_type": "text"
      },
      "source": [
        "### Concat Mode "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlEMn4CDcVdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "  \"decoder\": \"Bahdanau\",\n",
        "  \"encoder\": \"RNN\",\n",
        "  \"n_channels\": 4,\n",
        "  \"encoder_hidden\": 64,\n",
        "  \"encoder_layers\": 1,\n",
        "  \"encoder_dropout\": 0.2,\n",
        "  \"bidirectional_encoder\": False,\n",
        "  \"decoder_hidden\": 64,\n",
        "  \"decoder_layers\": 1,\n",
        "  \"decoder_dropout\": 0.2,\n",
        "  \"n_classes\":dataset.VOCAB_SIZE+3 ,\n",
        "  \"batch_size\": 32,\n",
        "  \"embedding_dim\": 64,\n",
        "  \"attention_score\": \"concat\",\n",
        "  \"learning_rate\": 0.001,\n",
        "  \"gpu\": True,\n",
        "  \"loss\": \"cross_entropy\"\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvMOke61cbvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Seq2Seq(config)\n",
        "model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCRpTxKVcc__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"learning_rate\", .001))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjbdwe3jcfFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  model,optimizer  = train(model,optimizer, train_loader,epoch,epochs)\n",
        "  evaluate(model,eval_loader)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhnkBB82chid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_plot(model,'cbada')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMzGeSiUdZRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}